{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lane Detection in Video.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6YET_H_zExY"
      },
      "source": [
        "# Detecting road lane lines in images and videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHOQpINS_5aW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6OUnWqgzxf1"
      },
      "source": [
        "The images and videos used in this project are taken from Udacity dataset on github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHoGlezoAE0n"
      },
      "source": [
        "!git clone https://github.com/udacity/CarND-LaneLines-P1.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QkqN3ifz1yz"
      },
      "source": [
        "Defining a helper function for displaying images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud1LjVGiAUku"
      },
      "source": [
        "def show_images(images, cmap=None):\n",
        "    cols = 2\n",
        "    rows = (len(images)+1)//cols\n",
        "    \n",
        "    plt.figure(figsize=(10, 11))\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        # use gray scale color map if there is only one channel\n",
        "        cmap = 'gray' if len(image.shape)==2 else cmap\n",
        "        plt.imshow(image, cmap=cmap)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bazVRE8wAaPw"
      },
      "source": [
        "test_images = [plt.imread(path) for path in glob.glob('/content/CarND-LaneLines-P1/test_images/*.jpg')]\n",
        "\n",
        "show_images(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U299cykE6T6"
      },
      "source": [
        "The lines in the images are white and yellow, to make things easier we'd like to extract only these colors. We'll experiment with different color spaces to do this and apply thresholding, to extract just the colors closely matching the lanes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUQMPKVADIu8"
      },
      "source": [
        "def yellow_white_selection(image):\n",
        "  lowerbw = np.uint8([200, 200, 200]) #lower threshold for white color\n",
        "  upperbw = np.uint8([255, 255, 255]) #upper threshold for white color\n",
        "  white_mask = cv2.inRange(image, lowerbw, upperbw) \n",
        "\n",
        "  lowerby = np.uint8([190, 190, 0]) #lower threshold for yellow\n",
        "  upperby = np.uint8([255, 255, 255]) #upper threshold for yellow\n",
        "  yellow_mask = cv2.inRange(image, lowerby, upperby)\n",
        "\n",
        "  #combining the two masks using bitwise operations\n",
        "  mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "  masked = cv2.bitwise_or(image,image, mask=mask)\n",
        "\n",
        "  return masked\n",
        "\n",
        "show_images(list(map(yellow_white_selection, test_images)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoTjj_OAFweu"
      },
      "source": [
        "Almost all the lanes are visible except for those shaded by an object. We may need to use a different color space and try to extract the lines then."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHlwagxUF_xI"
      },
      "source": [
        "HLS (Hue, Light, Saturation) color space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKeo2bK1F-j2"
      },
      "source": [
        "def to_hls(image):\n",
        "  return cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
        "show_images(list(map(to_hls, test_images)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOzWzl2RGzo1"
      },
      "source": [
        "Now, all the lanes are clearly visible. The lanes changes colors - white to green and yellow to blue. We need to take it into consideration when defining a function for extracting these lines to make masks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi8sjADLHELt"
      },
      "source": [
        "def yellow_white_hls_selection(image):\n",
        "  converted = to_hls(image)\n",
        "  lowerbw = np.uint8([0, 200, 0]) #white lower boundary\n",
        "  upperbw = np.uint8([255, 255, 255]) #white upper boundary\n",
        "  white_mask = cv2.inRange(converted, lowerbw, upperbw)\n",
        "\n",
        "  lowerby = np.uint8([10, 0, 100]) #yellow lower boundary\n",
        "  upperby = np.uint8([40, 255, 255]) #yellow upper boundary (we can experiment with these)\n",
        "  yellow_mask = cv2.inRange(converted, lowerby, upperby)\n",
        "\n",
        "  #Combining the two masks by bitwise\n",
        "  mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "  masked_hls = cv2.bitwise_and(image, image, mask=mask)\n",
        "  return masked_hls\n",
        "\n",
        "hls_images = list(map(yellow_white_hls_selection, test_images))\n",
        "show_images(hls_images)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVhOoDXJaog"
      },
      "source": [
        "Canny Edge Detection\n",
        "\n",
        "\n",
        "*   convert images into gray scale\n",
        "*   smooth the edges with Gaussian Blur\n",
        "*   use cv2.Canny() for detecting the edges\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSK3ytmIOn0B"
      },
      "source": [
        "HLS images into gray scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkROfKmtJZXu"
      },
      "source": [
        "def to_gray(image):\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "  return gray\n",
        "\n",
        "hls_to_gray_images = list(map(to_gray, hls_images))\n",
        "show_images(hls_to_gray_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npD8NAi6PnY5"
      },
      "source": [
        "# Smoothing the edges\n",
        "\n",
        "`cv2.GaussianBlur()` function requires `kernel_size` parameter, the number must be odd. The bigger the number, the blurrier the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOXqKpmrP-Dz"
      },
      "source": [
        "def smoothing(image, kernel_size=5):\n",
        "  smoothed = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
        "  return smoothed\n",
        "\n",
        "smoothed_images = list(map(smoothing, hls_to_gray_images))\n",
        "show_images(smoothed_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqpGXP7BRQOq"
      },
      "source": [
        "# Canny Edge Detection\n",
        "\n",
        "The `Canny` function takes two thresholds as parameters. To find their best values we need to remember:\n",
        "\n",
        "\n",
        "*   If a pixel gradient is higher than the upper threshold, the pixel is accepted as an edge.\n",
        "*   If a pixel gradient value is below the lower threshold, then it is rejected.\n",
        "*   If the pixel gradient is between the two thresholds, then it will be accepted only if it is connected to a pixel that is above the upper threshold.\n",
        "*   Canny recommended the upper:lower ratio between 2:1 and 3:1.\n",
        "*   If the high threshold is too high, we'll see no edges.\n",
        "*   If the low threshold is too low, we'll see too much noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozu0vsKiRRM8"
      },
      "source": [
        "def canny_edges(image, low_threshold = 90, high_threshold=180):\n",
        "  edges = cv2.Canny(image, low_threshold, high_threshold)\n",
        "  return edges\n",
        "\n",
        "canny_images = list(map(canny_edges, smoothed_images))\n",
        "show_images(canny_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjPbCnSzUb2L"
      },
      "source": [
        "# Selecting the region of interest\n",
        "\n",
        "After we've detected the edges in the picture we're going to to choose the region of interest by contouring the road. We'll specify the vertices surrounding the road and only use this region for lane detection. This way we'll get rid of cannied cars, trees etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTbGGwK9UaPq"
      },
      "source": [
        "def region_of_interest(image, vertices):\n",
        "  copy = np.zeros_like(image)\n",
        "  if len(copy.shape)==2:\n",
        "    cv2.fillPoly(copy, vertices, 255)\n",
        "  else:\n",
        "    cv2.fillPoly(copy, vertices, (255,)*copy.shape[2]) # in case, the input image has a channel dimension        \n",
        "  return cv2.bitwise_and(image, copy)\n",
        "\n",
        "def vertices(image):\n",
        "\n",
        "  # first, define the polygon by vertices\n",
        "    rows, cols = image.shape[:2]\n",
        "    bottom_left  = [cols*0.1, rows*0.95]\n",
        "    top_left     = [cols*0.4, rows*0.6]\n",
        "    bottom_right = [cols*0.9, rows*0.95]\n",
        "    top_right    = [cols*0.6, rows*0.6] \n",
        "    # the vertices are an array of polygons (i.e array of arrays) and the data type must be integer\n",
        "    vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
        "    return region_of_interest(image, vertices)\n",
        "\n",
        "region_images = list(map(vertices, canny_images))\n",
        "show_images(region_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpUNxZeoXApq"
      },
      "source": [
        "# Hough Transform\n",
        "Now we need to recongnize the lanes in an image. To do this we'll use `cv2.HoughLinesP()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQoUt6tbW_Bw"
      },
      "source": [
        "def hough_lines(image):\n",
        "  # Define the Hough transform parameters\n",
        "  # image is the output of our region selection\n",
        "    rho = 1 # distance resolution in pixels of the Hough grid\n",
        "    theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
        "    threshold = 20     # minimum number of votes (intersections in Hough grid cell)\n",
        "    min_line_length = 20 #minimum number of pixels making up a line\n",
        "    max_line_gap = 300    # maximum gap in pixels between connectable line segments\n",
        "    line_image = np.copy(image)*0 # creating a blank to draw lines on\n",
        "\n",
        "    # Run Hough on edge detected image\n",
        "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
        "    lines = cv2.HoughLinesP(image, rho, theta, threshold, np.array([]),\n",
        "                                min_line_length, max_line_gap)\n",
        "    return lines\n",
        "\n",
        "list_of_lines = list(map(hough_lines, region_images ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODWTZoOhZQsj"
      },
      "source": [
        "# Drawing detected lines into the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5g3w0R_Yq9S"
      },
      "source": [
        "Function `hough_lines` gives us a list of lines, not an image. To see them in an image we need to draw them first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntGhICjwZXSE"
      },
      "source": [
        "def draw_lines(image, lines, color=(255, 0, 0), thickness=2):\n",
        "  image = np.copy(image)\n",
        "  # Iterate over the output \"lines\" and draw lines on a blank image\n",
        "  for line in lines:\n",
        "      for x1,y1,x2,y2 in line:\n",
        "           cv2.line(image,(x1,y1),(x2,y2),color, thickness)\n",
        "  return image\n",
        "\n",
        "line_images = []\n",
        "for image, lines in zip(test_images, list_of_lines):\n",
        "    line_images.append(draw_lines(image, lines))\n",
        "\n",
        "show_images(line_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxry_Lkxf8DP"
      },
      "source": [
        "# Extrapolating the lanes\n",
        "The lanes in the pictures are not continuous. We'll extrapolate them. Each line is a function. So, first we'll define what is left and right lane. The feature that enables us to do this is the slope of a function. If the slope is > than 0, the function is positive, if the slope < 0 it is negative. Since the pixel matrix is reveresed (it's a mirror reflection by the x axis), left line will be negative, and right one positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdBVHqqehN_J"
      },
      "source": [
        "def average_slope_intercept(lines):\n",
        "    left_lines    = [] # (slope, intercept)\n",
        "    left_weights  = [] # (length,)\n",
        "    right_lines   = [] # (slope, intercept)\n",
        "    right_weights = [] # (length,)\n",
        "    \n",
        "    for line in lines:\n",
        "        for x1, y1, x2, y2 in line:\n",
        "            if x2==x1:\n",
        "                continue # ignore a vertical line\n",
        "            slope = (y2-y1)/(x2-x1)\n",
        "            intercept = y1 - slope*x1\n",
        "            length = np.sqrt((y2-y1)**2+(x2-x1)**2)\n",
        "            if slope < 0: # y is reversed in image\n",
        "                left_lines.append((slope, intercept))\n",
        "                left_weights.append((length))\n",
        "            else:\n",
        "                right_lines.append((slope, intercept))\n",
        "                right_weights.append((length))\n",
        "    \n",
        "    # add more weight to longer lines    \n",
        "    left_lane  = np.dot(left_weights,  left_lines) /np.sum(left_weights)  if len(left_weights) >0 else None\n",
        "    right_lane = np.dot(right_weights, right_lines)/np.sum(right_weights) if len(right_weights)>0 else None\n",
        "    \n",
        "    return left_lane, right_lane # (slope, intercept), (slope, intercept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeN-MnKt0y9L"
      },
      "source": [
        "Now, we need to convert a line represented in slope and intercept into pixel points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK_20ooHhjPk"
      },
      "source": [
        "def make_line_points(y1, y2, line):\n",
        "    if line is None:\n",
        "        return None\n",
        "    \n",
        "    slope, intercept = line\n",
        "    \n",
        "    # make sure everything is integer as cv2.line requires it\n",
        "    x1 = int((y1 - intercept)/slope)\n",
        "    x2 = int((y2 - intercept)/slope)\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    \n",
        "    return ((x1, y1), (x2, y2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpVcz-Uv08P1"
      },
      "source": [
        "Now, we need to connect the dots and draw extrapolated lines in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG4of9x3h_V8"
      },
      "source": [
        "def lane_lines(image, lines):\n",
        "    left_lane, right_lane = average_slope_intercept(lines)\n",
        "    \n",
        "    y1 = image.shape[0] # bottom of the image\n",
        "    y2 = y1*0.6         # slightly lower than the middle\n",
        "\n",
        "    left_line  = make_line_points(y1, y2, left_lane)\n",
        "    right_line = make_line_points(y1, y2, right_lane)\n",
        "    \n",
        "    return left_line, right_line\n",
        "\n",
        "    \n",
        "def draw_lane_lines(image, lines, color=[255, 0, 0], thickness=15):\n",
        "    # make a separate image to draw lines on\n",
        "    line_image = np.zeros_like(image)\n",
        "    for line in lines:\n",
        "        if line is not None:\n",
        "            cv2.line(line_image, *line,  color, thickness)\n",
        "    # image1 * α + image2 * β + λ\n",
        "    # image1 and image2 must be the same shape.\n",
        "    final = cv2.addWeighted(image, 1.0, line_image, 0.95, 0.0)\n",
        "    return final\n",
        "             \n",
        "    \n",
        "lane_images = []\n",
        "for image, lines in zip(test_images, list_of_lines):\n",
        "    lane_images.append(draw_lane_lines(image, lane_lines(image, lines)))\n",
        "\n",
        "    \n",
        "show_images(lane_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlPhiQRy2EGD"
      },
      "source": [
        "All functions for lane detection have been defined. `image_pipeline` will combine them all together. It takes three parameters: image, image_path and outpath. We'll need paths to lated save an image in a folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CbJeTf8cL5v"
      },
      "source": [
        "#Working Pipline\n",
        "def image_pipeline(image, image_path, outpath):\n",
        "  hls_image = yellow_white_hls_selection(image)\n",
        "  hls_to_gray_image = to_gray(hls_image)\n",
        "  smoothed_image = smoothing(hls_to_gray_image)\n",
        "  canny_image = canny_edges(smoothed_image)\n",
        "  region_image = vertices(canny_image)\n",
        "  lines = hough_lines(region_image)\n",
        "  left_line, right_line = lane_lines(image, lines)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  final = draw_lane_lines(image, (left_line, right_line))\n",
        "  my_dpi=96\n",
        "  plt.figure(figsize=(960/my_dpi, 540/my_dpi), dpi=my_dpi)\n",
        "  final_image = plt.imshow(final)\n",
        "  plt.axis('off')\n",
        "  save_fname = os.path.join(outpath, os.path.basename(image_path))\n",
        "  plt.savefig(save_fname, bbox_inches='tight', pad_inches=0, transparent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "576RStwP2HhA"
      },
      "source": [
        "Testing the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbzuvCo5cfS4"
      },
      "source": [
        "image = cv2.imread('/content/CarND-LaneLines-P1/test_images/solidWhiteRight.jpg')\n",
        "image_path = '/content/CarND-LaneLines-P1/test_images/solidWhiteRight.jpg'\n",
        "outpath = '/content/CarND-LaneLines-P1'\n",
        "image_pipeline(image, image_path, outpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M33MkKfQ2LMB"
      },
      "source": [
        "# Defining helper functions for processing videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3lAYmR2Rty"
      },
      "source": [
        "`clip_to_frame` takes any video and divides it into seperate frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiSAFsjatiGl"
      },
      "source": [
        "def clip_to_frame(vid_path, save_path):\n",
        "  vidcap = cv2.VideoCapture(vid_path)\n",
        "  success,image = vidcap.read()\n",
        "  count = 100\n",
        "  while success:\n",
        "    cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file \n",
        "    os.chdir(save_path)     \n",
        "    success,image = vidcap.read()\n",
        "    print('Read a new frame: ', success)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nnHm6iG2a8g"
      },
      "source": [
        "`clip_to_detected` incorporates `clip_to_frame` and `image_pipeline` for all frames in a given video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ao1aOZdtoCY"
      },
      "source": [
        "def clip_to_detected(vid_path, save_path, detected_path):\n",
        "  clip_to_frame(vid_path, save_path)\n",
        "  count= 0\n",
        "  for file in os.listdir(save_path):\n",
        "    image = cv2.imread(file)\n",
        "    image_pipeline(image, file, detected_path)\n",
        "    count=+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-43EWf92qyG"
      },
      "source": [
        "# Testing the method on a video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1-w-Rlw0rp"
      },
      "source": [
        "parent= '/content/CarND-LaneLines-P1'\n",
        "os.mkdir(os.path.join(parent, 'Frames White Right'))\n",
        "os.mkdir(os.path.join(parent, 'White Right Detected'))\n",
        "vid_path='/content/CarND-LaneLines-P1/test_videos/solidWhiteRight.mp4'\n",
        "save_path='/content/CarND-LaneLines-P1/Frames White Right'\n",
        "out_path='/content/CarND-LaneLines-P1/White Right Detected'\n",
        "\n",
        "clip_to_detected(vid_path, save_path, out_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0_36Hw320BC"
      },
      "source": [
        "Make video out of images with detected lane lines and save into a folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umq4p-LhxDvG"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import natsort\n",
        "\n",
        "sorted(os.listdir('/content/CarND-LaneLines-P1/White Right Detected'))\n",
        "\n",
        "image_folder = '/content/CarND-LaneLines-P1/White Right Detected'\n",
        "video_name = '/content/CarND-LaneLines-P1/video.avi'\n",
        "\n",
        "images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\")]\n",
        "natsort.natsorted(images)\n",
        "images.sort()\n",
        "frame = cv2.imread(os.path.join(image_folder, images[1]))\n",
        "height, width, layers = frame.shape\n",
        "\n",
        "video = cv2.VideoWriter(video_name, 0, 30, (width,height))\n",
        "\n",
        "for image in images:\n",
        "    video.write(cv2.imread(os.path.join(image_folder, image)))\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "video.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}